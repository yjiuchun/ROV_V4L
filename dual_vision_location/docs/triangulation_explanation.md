# 双目三角测量原理详解

## 一、目的

三角测量的目的是**从左右两个相机图像中的配对像素坐标，计算出对应3D点在空间中的实际坐标**。

### 主要应用：
1. **深度估计**：获取场景中每个点的深度信息
2. **3D重建**：恢复场景的三维结构
3. **距离测量**：计算物体到相机的距离
4. **位姿估计**：结合已知3D点坐标，可以估计相机或目标的位姿

## 二、基本原理

### 1. 视差（Disparity）概念

**视差**是指同一个3D点在左右两个相机图像中的水平位置差：

```
视差 = 左图像中的x坐标 - 右图像中的x坐标
```

### 2. 深度与视差的关系

根据相似三角形原理：

```
深度 Z = (基线长度 × 焦距) / 视差
```

其中：
- **基线长度（Baseline）**：左右两个相机光心之间的距离
- **焦距（f）**：相机的焦距（像素单位）
- **视差（d）**：左右图像中对应点的x坐标差

### 3. 几何关系图

```
左相机光心(O_L)    3D点P(X,Y,Z)
    |                /
    |               /
    |              /
    |             /
    |            /
    |           /
    |          /
    | 基线B   /
    |       /
    |      /
右相机光心(O_R)
```

当3D点P投影到左右图像时：
- 左图像：像素坐标 (x_L, y)
- 右图像：像素坐标 (x_R, y)
- 视差：d = x_L - x_R

## 三、具体步骤

### 步骤1：相机标定

获取以下参数：
- **左相机内参矩阵 K_L**：包含焦距 (fx, fy) 和主点 (cx, cy)
- **右相机内参矩阵 K_R**：包含焦距和主点
- **立体标定参数**：
  - 旋转矩阵 **R**：从右相机到左相机的旋转
  - 平移向量 **T**：从右相机到左相机的平移（基线向量）

### 步骤2：图像特征点检测和匹配

在左右图像中分别检测特征点，并确保一一对应：
- 左图像点：`(x_L1, y_L1), (x_L2, y_L2), ...`
- 右图像点：`(x_R1, y_R1), (x_R2, y_R2), ...`

**注意**：对应点的y坐标应该相同（或非常接近），因为两个相机在同一水平线上。

### 步骤3：构建投影矩阵

根据标定参数构建左右相机的投影矩阵：

```
P_L = K_L × [I | 0]           (左相机投影矩阵)
P_R = K_R × [R | T]           (右相机投影矩阵)
```

其中：
- `I` 是3×3单位矩阵
- `0` 是3×1零向量
- `[R | T]` 是外参矩阵

### 步骤4：执行三角测量

使用OpenCV的 `triangulatePoints()` 函数：

```python
# 输入：
# - 左相机投影矩阵 P_L
# - 右相机投影矩阵 P_R
# - 左图像像素坐标点集 left_points
# - 右图像像素坐标点集 right_points

# 输出：
# - 3D点坐标（齐次坐标或非齐次坐标）
points_3d = cv2.triangulatePoints(P_L, P_R, left_points, right_points)
```

### 步骤5：坐标转换

三角测量得到的是**齐次坐标** (X, Y, Z, W)，需要转换为**非齐次坐标**：

```
X_3D = X / W
Y_3D = Y / W
Z_3D = Z / W
```

得到的坐标是**左相机坐标系**中的3D坐标。

## 四、数学公式详解

### 1. 投影方程

对于左相机：
```
[u_L, v_L, 1]^T = K_L × [X_c, Y_c, Z_c]^T
```

对于右相机：
```
[u_R, v_R, 1]^T = K_R × R × [X_c, Y_c, Z_c]^T + K_R × T
```

其中 `(X_c, Y_c, Z_c)` 是3D点在相机坐标系中的坐标。

### 2. 三角测量求解

给定左右图像的像素坐标，我们需要求解3D坐标。这是一个超定方程组：

对于每个点对，我们有：
```
u_L = fx_L × (X_c / Z_c) + cx_L
v_L = fy_L × (Y_c / Z_c) + cy_L
u_R = fx_R × (R11×X_c + R12×Y_c + R13×Z_c + Tx) / Z_R + cx_R
v_R = fy_R × (R21×X_c + R22×Y_c + R23×Z_c + Ty) / Z_R + cy_R
```

通过最小二乘法或SVD分解求解得到 `(X_c, Y_c, Z_c)`。

## 五、在您的代码中的应用

在 `stereo_triangulation.py` 中的实现：

```python
def triangulate_points(self, left_points, right_points):
    # 1. 确保输入格式正确
    left_points = np.array(left_points, dtype=np.float32).reshape(-1, 1, 2)
    right_points = np.array(right_points, dtype=np.float32).reshape(-1, 1, 2)
    
    # 2. 使用OpenCV的三角测量函数
    points_4d = cv2.triangulatePoints(
        self.left_proj_matrix,    # 左相机投影矩阵
        self.right_proj_matrix,   # 右相机投影矩阵
        left_points,              # 左图像像素坐标
        right_points              # 右图像像素坐标
    )
    
    # 3. 转换为非齐次坐标
    points_3d = points_4d[:3] / points_4d[3]  # (X, Y, Z) = (X, Y, Z, W) / W
    points_3d = points_3d.T                   # 转置为 (N, 3)
    
    return points_3d
```

## 六、精度影响因素

1. **基线长度**：基线越长，深度估计精度越高，但视野重叠区域越小
2. **标定精度**：相机内参和外参的标定精度直接影响结果
3. **特征点匹配精度**：左右图像中点配对必须准确
4. **视差测量精度**：视差误差会导致深度误差（深度与视差平方成反比）

## 七、示例

假设：
- 基线长度：0.12米（12厘米）
- 焦距：476.7像素
- 左图像点：(640, 360)
- 右图像点：(620, 360)
- 视差：640 - 620 = 20像素

深度计算：
```
Z = (0.12 × 476.7) / 20 = 2.86米
```

这就是三角测量计算出的该3D点的深度（Z坐标）。

## 八、后续应用

得到3D坐标后，可以：

1. **直接使用3D坐标**：了解场景中点的空间位置
2. **PnP位姿估计**：如果知道这些点在某个参考坐标系中的坐标，可以使用PnP算法估计相机或目标的位姿
3. **3D重建**：将多个点的3D坐标组合起来，重建场景的三维模型


